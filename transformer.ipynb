{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03c470c",
   "metadata": {},
   "source": [
    "# Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a53b5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98753578",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "![Model Architecture](images/model_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7a44c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7265e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Define standard linear + softmax generation step.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d45b7",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Stacks\n",
    "Both encoder and decoder are composed of a stack of N=6 identical layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7cf77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone(module, N):\n",
    "    \"\"\"\n",
    "    Produce N identical layers.\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf470cad",
   "metadata": {},
   "source": [
    "### Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4003a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Core encoder is a stack of N layers\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layers = clone(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae30f35",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "employ a residual connection around each of the two sub-layers, followed by layer normalization.\n",
    "\n",
    " $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c12577cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNorm Module\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.b_2 = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52e614",
   "metadata": {},
   "source": [
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=512$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8765427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, dropout):\n",
    "        super(SublayerConnection,self).__init__()\n",
    "        self.norm = LayerNorm(normalized_shape)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b0651",
   "metadata": {},
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "738ac5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder is made up of self-attention and feed forward\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, mha, ffn, dropout):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.mha = mha\n",
    "        self.ffn = ffn\n",
    "        self.sublayer = clones(SublayerConnection(normalized_shape, dropout), 2)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.mha(x, x, x, mask))\n",
    "        return self.sublayer[1](x,self.ffn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f55731",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bde7f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder is a stack of N layers with masking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048471d",
   "metadata": {},
   "source": [
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e1e8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder is made of self-attn, src-attn, and feed forward\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, self_attn, src_attn, ffn, dropout):\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.ffn = ffn\n",
    "        self.sublayer = clones(SublayerConnection(normalized_shape, dropout), 3)\n",
    "    \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        enc_output = enc_output\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, padding_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, enc_output, enc_output, look_ahead_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea4e5a",
   "metadata": {},
   "source": [
    "This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae6c03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(seq):\n",
    "    \"\"\"\n",
    "    Mask out subsequent positions\n",
    "    \"\"\"\n",
    "    _, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23207d",
   "metadata": {},
   "source": [
    "## Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f01fe3",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca7f51",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/scaled_dot_product_attention.png\" style=\"zoom:33%;\" />\n",
    "$$                                                                       \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cd24da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None, dropout=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn,v), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce8d28",
   "metadata": {},
   "source": [
    "### MultiHeadedAttention\n",
    "\n",
    "<img src=\"images/multiheaded_attention.png\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f14ec",
   "metadata": {},
   "source": [
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$$    \n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
    "$$\n",
    "\n",
    "Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$. \n",
    "\n",
    "In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b508d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clone(nn.Linear(d_model,d_model), 4)\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        sz_b = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model ( sz_b * len_q * (h*d_k) => sz_b * h * len_q *d_k )\n",
    "        query, key, value = [l(x).view(sz_b, -1, self.h, self.d_k).transpose(1,2) \n",
    "                             for l, x in zip(linears,(query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(sz_b, -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.linears[-1](x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1fc55",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23aeb4",
   "metadata": {},
   "source": [
    "   \n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2 $$                                                                                                                          \n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1442ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN\"\n",
    "    \n",
    "    # TODO if there have the dropout ?\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1272fce3",
   "metadata": {},
   "source": [
    "## Embeddings and Softmax                                                                                                                                                                                                                                                                                           \n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [(cite)](https://arxiv.org/abs/1608.05859). In the embedding layers, we multiply those weights by $\\sqrt{d_{\\text{model}}}$.                                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b279a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the input tokens and output tokens to vectors of dimension d_model\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings,self).__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.emb(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582fe10f",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$  \n",
    "\n",
    "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e3f2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the PE function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,d_model, n_position=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.register_buffer('pe', self.__get_positional_encoding(n_position,d_model))\n",
    "        \n",
    "    def __get_positional_encoding(self, n_position, d_model):\n",
    "        pe = torch.zeros(n_position, d_model)\n",
    "        position = torch.arange(n_position).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)*\n",
    "                             -(math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e549b8",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b3d94fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequence to sequence model with attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "795c3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a7164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
